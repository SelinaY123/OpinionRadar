"""
å®Œæ•´æŠ–éŸ³åˆ†æç¤ºä¾‹ - æ¼”ç¤ºæ‰€æœ‰åŠŸèƒ½
"""

import sys
import os
sys.path.append('../../src')

import pandas as pd
import json
from datetime import datetime

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from analyzer.basic_analysis import analyze_comments
from analyzer.sentiment_analysis import SentimentAnalyzer
from analyzer.text_mining import TextMiner
from analyzer.visualization import DataVisualizer
from utils.logger import get_logger

def run_full_analysis(json_file):
    """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger = get_logger()
    logger.info("å¼€å§‹å®Œæ•´æŠ–éŸ³åˆ†ææµç¨‹")
    
    print("ğŸ¬ æŠ–éŸ³è¯„è®ºå®Œæ•´åˆ†æå·¥å…·")
    print("=" * 60)
    
    # æ­¥éª¤1ï¼šåŠ è½½æ•°æ®
    logger.log_step(1, "åŠ è½½æ•°æ®", f"æ–‡ä»¶: {json_file}")
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    df = pd.DataFrame(data['comments'])
    print(f"âœ“ åŠ è½½æ•°æ®: {len(df)} æ¡è¯„è®º")
    
    # æ­¥éª¤2ï¼šåŸºç¡€åˆ†æ
    logger.log_step(2, "åŸºç¡€åˆ†æ")
    print("\nğŸ“Š åŸºç¡€ç»Ÿè®¡åˆ†æ:")
    print("-" * 40)
    
    if 'ç‚¹èµæ•°' in df.columns:
        print(f"æ€»ç‚¹èµæ•°: {df['ç‚¹èµæ•°'].sum():,}")
        print(f"å¹³å‡ç‚¹èµ: {df['ç‚¹èµæ•°'].mean():.1f}")
    
    print(f"ç”¨æˆ·æ•°é‡: {df['ç”¨æˆ·'].nunique()}")
    print(f"è¯„è®ºæ—¶é—´èŒƒå›´: {df['æ—¶é—´'].min()} åˆ° {df['æ—¶é—´'].max()}")
    
    # æ­¥éª¤3ï¼šæƒ…æ„Ÿåˆ†æ
    logger.log_step(3, "æƒ…æ„Ÿåˆ†æ")
    print("\nğŸ’– æƒ…æ„Ÿåˆ†æ:")
    print("-" * 40)
    
    analyzer = SentimentAnalyzer()
    df, sentiment_counts = analyzer.analyze_comments(df)
    
    # æ­¥éª¤4ï¼šæ–‡æœ¬æŒ–æ˜
    logger.log_step(4, "æ–‡æœ¬æŒ–æ˜")
    print("\nğŸ” æ–‡æœ¬æŒ–æ˜:")
    print("-" * 40)
    
    miner = TextMiner()
    word_result = miner.analyze_comments(df, top_n=15)
    
    # å‘ç°çƒ­ç‚¹è¯é¢˜
    hot_topics = miner.find_hot_topics(df, min_count=2)
    if hot_topics:
        print("\nğŸ”¥ çƒ­ç‚¹è¯é¢˜:")
        for i, topic in enumerate(hot_topics[:3], 1):
            print(f"{i}. {topic['å…³é”®è¯']} (å‡ºç°{topic['å‡ºç°æ¬¡æ•°']}æ¬¡)")
    
    # æ­¥éª¤5ï¼šå¯è§†åŒ–
    logger.log_step(5, "å¯è§†åŒ–")
    print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨:")
    print("-" * 40)
    
    visualizer = DataVisualizer()
    
    analysis_results = {
        'word_counts': word_result,
        'sentiment_counts': sentiment_counts
    }
    
    charts = visualizer.generate_report(df, analysis_results)
    print(f"ç”Ÿæˆå›¾è¡¨: {len(charts)} ä¸ª")
    
    # æ­¥éª¤6ï¼šä¿å­˜ç»“æœ
    logger.log_step(6, "ä¿å­˜ç»“æœ")
    print("\nğŸ’¾ ä¿å­˜åˆ†æç»“æœ:")
    print("-" * 40)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"full_analysis_{timestamp}.xlsx"
    
    # å‡†å¤‡å¤šä¸ªå·¥ä½œè¡¨çš„æ•°æ®
    sheets_data = {
        'åŸå§‹æ•°æ®': df,
        'çƒ­é—¨è¯„è®º': df.nlargest(10, 'ç‚¹èµæ•°') if 'ç‚¹èµæ•°' in df.columns else df.head(10),
        'æƒ…æ„Ÿåˆ†æ': df[['ç”¨æˆ·', 'å†…å®¹', 'æƒ…æ„Ÿåˆ†æ•°', 'æƒ…æ„Ÿåˆ†ç±»']],
        'æ´»è·ƒç”¨æˆ·': df['ç”¨æˆ·'].value_counts().reset_index()
    }
    
    # ä¿å­˜ä¸ºExcel
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        for sheet_name, sheet_df in sheets_data.items():
            sheet_df.to_excel(writer, sheet_name=sheet_name[:31], index=False)
    
    print(f"âœ“ åˆ†æç»“æœå·²ä¿å­˜: {output_file}")
    
    # æ­¥éª¤7ï¼šç”ŸæˆæŠ¥å‘Š
    logger.log_step(7, "ç”ŸæˆæŠ¥å‘Š")
    print("\nğŸ“‹ åˆ†ææŠ¥å‘Šæ‘˜è¦:")
    print("-" * 40)
    
    report = f"""
æŠ–éŸ³è¯„è®ºåˆ†ææŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ•°æ®æ–‡ä»¶: {json_file}

ğŸ“Š æ•°æ®æ¦‚è§ˆ
- è¯„è®ºæ€»æ•°: {len(df):,} æ¡
- ç”¨æˆ·æ•°é‡: {df['ç”¨æˆ·'].nunique()} äºº
- æ—¶é—´èŒƒå›´: {df['æ—¶é—´'].min()} åˆ° {df['æ—¶é—´'].max()}

ğŸ’– æƒ…æ„Ÿåˆ†æ
- ç§¯æè¯„è®º: {len(df[df['æƒ…æ„Ÿåˆ†ç±»'] == 'ç§¯æ'])} æ¡
- ä¸­æ€§è¯„è®º: {len(df[df['æƒ…æ„Ÿåˆ†ç±»'] == 'ä¸­æ€§'])} æ¡
- æ¶ˆæè¯„è®º: {len(df[df['æƒ…æ„Ÿåˆ†ç±»'] == 'æ¶ˆæ'])} æ¡

ğŸ” æ–‡æœ¬åˆ†æ
- æ€»è¯æ±‡é‡: {word_result.get('total_words', 0):,} è¯
- ç‹¬ç‰¹è¯æ±‡: {word_result.get('unique_words', 0)} ä¸ª

ğŸ“ˆ è¾“å‡ºæ–‡ä»¶
1. ExcelæŠ¥å‘Š: {output_file}
2. å›¾è¡¨æ–‡ä»¶: {len(charts)} ä¸ªPNGå›¾è¡¨

ğŸ¯ åˆ†æå®Œæˆï¼
"""
    
    print(report)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = f"analysis_report_{timestamp}.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"å®Œæ•´åˆ†æå®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜: {output_file}")
    
    return df, output_file, charts

def main():
    """ä¸»å‡½æ•°"""
    print("æŠ–éŸ³è¯„è®ºå®Œæ•´åˆ†æç¤ºä¾‹")
    print("=" * 60)
    
    # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
    json_file = "../sample_data/sample_comments.json"
    
    if not os.path.exists(json_file):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
        print("æ­£åœ¨åˆ›å»ºç¤ºä¾‹æ•°æ®...")
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        example_data = {
            "video_info": {
                "video_id": "sample_video_001",
                "title": "æµ‹è¯•è§†é¢‘"
            },
            "comments": [
                {
                    "ç”¨æˆ·": "ç”¨æˆ·1",
                    "å†…å®¹": "è¿™ä¸ªè§†é¢‘çœŸçš„å¾ˆæ£’ï¼Œå†…å®¹å¾ˆæœ‰ä»·å€¼ï¼",
                    "ç‚¹èµæ•°": 150,
                    "æ—¶é—´": "2024-01-01 10:00:00"
                },
                {
                    "ç”¨æˆ·": "ç”¨æˆ·2", 
                    "å†…å®¹": "éå¸¸å–œæ¬¢ï¼Œå·²ç»åˆ†äº«ç»™æœ‹å‹äº†",
                    "ç‚¹èµæ•°": 80,
                    "æ—¶é—´": "2024-01-01 10:05:00"
                },
                {
                    "ç”¨æˆ·": "ç”¨æˆ·3",
                    "å†…å®¹": "å†…å®¹ä¸€èˆ¬ï¼Œæ²¡ä»€ä¹ˆæ–°æ„",
                    "ç‚¹èµæ•°": 20,
                    "æ—¶é—´": "2024-01-01 10:10:00"
                },
                {
                    "ç”¨æˆ·": "ç”¨æˆ·1",
                    "å†…å®¹": "æœŸå¾…æ›´å¤šè¿™æ ·çš„å¥½å†…å®¹",
                    "ç‚¹èµæ•°": 60,
                    "æ—¶é—´": "2024-01-01 10:15:00"
                },
                {
                    "ç”¨æˆ·": "ç”¨æˆ·4",
                    "å†…å®¹": "å¤ªå·®äº†ï¼Œæµªè´¹æ—¶é—´",
                    "ç‚¹èµæ•°": 5,
                    "æ—¶é—´": "2024-01-01 10:20:00"
                }
            ]
        }
        
        # ä¿å­˜ç¤ºä¾‹æ•°æ®
        os.makedirs(os.path.dirname(json_file), exist_ok=True)
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(example_data, f, ensure_ascii=False, indent=2)
        
        print(f"ç¤ºä¾‹æ•°æ®å·²åˆ›å»º: {json_file}")
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    try:
        df, output_file, charts = run_full_analysis(json_file)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"âœ“ æ•°æ®æ–‡ä»¶: {json_file}")
        print(f"âœ“ åˆ†ææŠ¥å‘Š: {output_file}")
        print(f"âœ“ ç”Ÿæˆå›¾è¡¨: {len(charts)} ä¸ª")
        print("=" * 60)
        
    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()